{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Machine Learning in Finance - Data Processing\n",
    "\n",
    "This notebook will focus on processing the data with ML techniques (RandomForest), before applying ML techniques to find the top characteristics for price prediction.\n",
    "\n",
    "***Note: This part used a package `missingpy` that requires old versions of scipy. To avoid unnecessary conflict, please uncomment the first cell to create a conda environment just for this pre-processing task. Do not forget to set the created environment as you IDE interpreter. Finally, `missingpy` has a second model which is deprecated and cause issues while importing. Then please follow the step of the video [here](https://www.youtube.com/watch?v=_886JGYt1Ts).***\n",
    "\n",
    "*Authors:* [Mina Attia](https://people.epfl.ch/mina.attia), [Arnaud Felber](https://people.epfl.ch/arnaud.felber), [Milos Novakovic](https://people.epfl.ch/milos.novakovic), [Rami Atassi](https://people.epfl.ch/rami.atassi) & [Paulo Ribeiro](https://people.epfl.ch/paulo.ribeirodecarvalho)"
   ],
   "id": "5588cd20f4297393"
  },
  {
   "cell_type": "code",
   "source": [
    "#!conda create --name impute python=3.8\n",
    "#!conda activate impute\n",
    "#!/opt/anaconda3/envs/impute/bin/pip install missingpy\n",
    "#!/opt/anaconda3/envs/impute/bin/pip install scikit_learn\n",
    "#!/opt/anaconda3/envs/impute/bin/pip install pandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T12:36:55.036289Z",
     "start_time": "2024-05-11T12:36:55.034485Z"
    }
   },
   "id": "57ba8eed7a3a1b8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9704e4adf07bf1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T12:36:57.158455Z",
     "start_time": "2024-05-11T12:36:56.743635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from helpers import load_data_df\n",
    "from data_processing import impute_data\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Filter out UserWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "824a0970c6e39a9f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "Load the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1ee5a5b3bdf118e"
  },
  {
   "cell_type": "code",
   "source": [
    "file_path = 'data/data_reduce.csv'\n",
    "\n",
    "data = load_data_df(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T12:36:58.835445Z",
     "start_time": "2024-05-11T12:36:58.826225Z"
    }
   },
   "id": "3a9f594a2762397f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Impute\n",
    "\n",
    "Retrieve the missing values using Machine Learning techniques. To do so, we use the API from `missingpy` and call the MissForest algorithm. MissForest imputes missing values using Random Forests in an iterative fashion."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T15:49:42.109828Z",
     "start_time": "2024-05-07T15:49:41.165028Z"
    }
   },
   "id": "b4c4fd00f1fbfba7"
  },
  {
   "cell_type": "code",
   "source": [
    "chunks = 10\n",
    "data_chunks = np.array_split(data, chunks)\n",
    "\n",
    "imputed_data_list = []\n",
    "for data_chunk in tqdm(data_chunks[1:], total=chunks):\n",
    "    imputed_data = impute_data(data=data_chunk.reset_index(drop=True),\n",
    "                               seed=42)\n",
    "    imputed_data_list.append(imputed_data)\n",
    "    \n",
    "imputed_data_full = pd.concat(imputed_data_list, ignore_index=True).drop(columns='Unnamed: 0')\n",
    "    \n",
    "# Check if there are any NaN values in the DataFrame\n",
    "if imputed_data_full.isna().any().any():\n",
    "    print(\"There still has NaN values in the DataFrame.\")\n",
    "else:\n",
    "    print(\"There are no NaN values in the DataFrame.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T12:37:00.565567Z",
     "start_time": "2024-05-11T12:37:00.553448Z"
    }
   },
   "id": "55d2a0f1d5d2e6f6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Processing\n",
    "\n",
    "Since no more Nan values are in price, we can compute the return of each record. To do so we compute the diff between every related record and divide by the current price. This way each record is associate to the return of the asset the next month. "
   ],
   "id": "5ea718e7ba98e030"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T12:37:02.422350Z",
     "start_time": "2024-05-11T12:37:02.411773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rename correctly the column\n",
    "imputed_data_full.rename(columns={'Price': 'log_price'}, inplace=True)\n",
    "\n",
    "# Position the log_price column at the end of the dataframe\n",
    "column_to_move = imputed_data_full.pop('log_price')\n",
    "imputed_data_full['log_price'] = column_to_move\n",
    "\n",
    "# Create the column price by takin the exponential of the log price\n",
    "imputed_data_full['price'] = np.exp(-1*imputed_data_full['log_price'])\n",
    "\n",
    "# Compute the log return and simple return\n",
    "# Sort the DataFrame by date\n",
    "imputed_data_full.sort_values(by=['permno', 'date'], inplace=True)\n",
    "\n",
    "# Calculate logarithmic returns for each asset separately\n",
    "imputed_data_full['log_diff'] = imputed_data_full.groupby('permno')['log_price'].diff()\n",
    "imputed_data_full['diff'] = imputed_data_full.groupby('permno')['price'].diff()\n",
    "\n",
    "# Shift the returns by one month to align with the next month's data\n",
    "imputed_data_full['log_diff'] = imputed_data_full.groupby('permno')['log_diff'].shift(-1)\n",
    "imputed_data_full['diff'] = imputed_data_full.groupby('permno')['diff'].shift(-1)\n",
    "\n",
    "# Drop the last row for each asset since it will have NaN due to shifting\n",
    "imputed_data_full = imputed_data_full.dropna()\n",
    "\n",
    "# Compute the return ratio now\n",
    "imputed_data_full['log_return'] = imputed_data_full['log_diff'] / imputed_data_full['log_price']\n",
    "imputed_data_full['return'] = imputed_data_full['diff'] / imputed_data_full['price']\n",
    "\n",
    "# Reset index if needed\n",
    "imputed_data_full.reset_index(drop=True, inplace=True)\n",
    "imputed_data_full.drop(columns=['diff', 'log_diff'], inplace=True)"
   ],
   "id": "fd415b3a663b4185",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store Impute Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9b34806877ec76d"
  },
  {
   "cell_type": "code",
   "source": "imputed_data_full.to_csv('data/imputed/data_imputed.csv', index=False)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T12:37:05.101150Z",
     "start_time": "2024-05-11T12:37:05.088216Z"
    }
   },
   "id": "26b18813e970310e",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
